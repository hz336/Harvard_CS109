{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science \n",
    "\n",
    "## Homework 1: Data Collection - Web Scraping - Data Parsing\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2018**<br/>\n",
    "**Instructors**: Pavlos Protopapas and Kevin Rader\n",
    "\n",
    "\n",
    "\n",
    "<hr style='height:2px'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## RUN THIS CELL TO GET THE RIGHT FORMATTING \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "- To submit your assignment follow the instructions given in Canvas.\n",
    "- The deliverables in Canvas are: <br/>\n",
    "    a) This python notebook with your code and answers, plus a pdf version of it (see Canvas for details),<br/>\n",
    "    b) the bibtex file you created, <br/>\n",
    "    c) The CSV file you created, <br/>\n",
    "    d) The JSON file you created. <br/>\n",
    "    \n",
    "- Exercise **responsible scraping**. Web servers can become slow or unresponsive if they receive too many requests from the same source in a short amount of time. Use a delay of 10 seconds between requests in your code. This helps not to get blocked by the target website. Run the webpage fetching part of the homework only once and do not re-run after you have saved the results in the JSON file (details below). \n",
    "- Web scraping requests can take several minutes. This is another reason why you should not wait until the last minute to do this homework.\n",
    "- For this assignment, we will use Python 3.5 for grading.\n",
    "\n",
    "\n",
    "# Data Collection - Web Scraping - Data Parsing \n",
    " \n",
    "\n",
    "In this homework, your goal is to learn how to acquire, parse, clean, and analyze data. Initially you will read the data from a file, and then later scrape them directly from a website. You will look for specific pieces of information by parsing the data, clean the data to prepare them for analysis, and finally, answer some questions.\n",
    "\n",
    "In doing so you will get more familiar with three of the common file formats for storing and transferring data, which are:\n",
    "- CSV, a text-based file format used for storing tabular data that are separated by some delimiter, usually comma or space.\n",
    "- HTML/XML, the stuff the web is made of.\n",
    "- JavaScript Object Notation (JSON), a text-based open standard designed for transmitting structured data over the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Help a professor parse their publications and extract information.\n",
    "\n",
    "### Overview\n",
    "\n",
    "In this part your goal is to parse the HTML page of a professor containing some of his/her publications, and answer some questions. This page is provided to you in the file `data/publist_super_clean.html`. There are 45 publications in descending order from No. 244 to No. 200.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this file provided\n",
    "PUB_FILENAME = 'data/publist_super_clean.html'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 1 [40 pts]: Parsing and Converting to bibTex and CSV using Beautiful Soup and python string manipulation </b></div>\n",
    "\n",
    "A lot of the bibliographic and publication information is displayed in various websites in a not-so-structured HTML files. Some publishers prefer to store and transmit this information in a .bibTex file which looks roughly like this (we've simplified a few things):\n",
    "```\n",
    "@article { \n",
    "     author = \"John Doyle\"\n",
    "     title = \"Interaction between atoms\"\n",
    "     URL = \"Papers/PhysRevB_81_085406_2010.pdf\"\n",
    "     journal = \"Phys. Rev. B\"\n",
    "     volume = \"81\"\n",
    "}\n",
    "```\n",
    "You will notice that this file format is a set of items, each of which is a set of key-value pairs. In the python world, you can think of this as a list of dictionaries.\n",
    "If you think about spreadsheets (as represented by CSV files), they have the same structure. Each line is an item, and has multiple features, or keys, as represented by that line's value for the column corresponding to the key.\n",
    "\n",
    "You are given an .html file containing a list of papers scraped from the author's website and you are to write the information into .bibTex and .CSV formats. A useful tool for parsing websites is BeautifulSoup  (http://www.crummy.com/software/BeautifulSoup/) (BS).  In this problem, will parse the  file using BS, which makes parsing HTML a lot easier.\n",
    "\n",
    "**1.1** Write a function called `make_soup` that accepts a filename for an HTML file and returns a BS object.\n",
    "    \n",
    "**1.2** Write a function that reads in the BS object, parses it, converts it into a list of dictionaries: one dictionary per paper. Each of these dictionaries should have the following format (with different values for each publication):\n",
    "```\n",
    "{'author': 'L.A. Agapito, N. Kioussis and E. Kaxiras',\n",
    " 'title': '\"Electric-field control of magnetism in graphene quantum dots:\\n Ab initio calculations\"',\n",
    " 'URL': 'Papers/PhysRevB_82_201411_2010.pdf',\n",
    " 'journal': 'Phys. Rev. B',\n",
    " 'volume': '82'}\n",
    "```\n",
    "\n",
    "\n",
    "**1.3** Convert the list of dictionaries into standard .bibTex format using python string manipulation, and write the results into a file called `publist.bib`.\n",
    "\n",
    "**1.4** Convert the list of dictionaries into standard tabular .csv format using pandas, and write the results into a file called `publist.csv`. The csv file should have a header and no integer index.\n",
    "\n",
    "    \n",
    "#### HINT \n",
    "- Inspect the HTML code for tags that indicate information chunks such as `title` of the paper.  The `find_all` method of BeautifulSoup might be useful.\n",
    "- Question 1.2 is better handled if you break the code into functions, each performing a small task such as finding the author(s) for each paper. \n",
    "- Question 1.3 is effectively tackled by first using python string formatting on a template string.\n",
    "- Make sure you catch exceptions when needed. \n",
    "- Make sure you check for **missing data** and handle these cases as you see fit. \n",
    "\n",
    "\n",
    "#### Resources\n",
    "- [BeautifulSoup Tutorial](https://www.dataquest.io/blog/web-scraping-tutorial-python/).\n",
    "- More about the [BibTex format](http://www.bibtex.org).<BR>\n",
    "    \n",
    "### Answers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries \n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class='sq'>**1.1 Write a function called `make_soup`  ...**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_soup(filename: str) -> BeautifulSoup: \n",
    "    '''Open the file and convert into a BS object. \n",
    "       \n",
    "       Args:\n",
    "           filename: A string name of the file.\n",
    "       \n",
    "       Returns:\n",
    "           A BS object containing the HTML page ready to be parsed.\n",
    "    '''\n",
    "    # your code here\n",
    "    with open(filename) as fdr:\n",
    "        data = fdr.read()\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "\n",
    "    return soup\n",
    "\n",
    "soup = make_soup(PUB_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 Write a function that reads in the BS object, parses it, converts it into a list of dictionaries...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing volume: 18\n",
      "<ol start=\"226\">\n",
      "<li>\n",
      "<a href=\"Papers/IEEE-SC10_2010.pdf\" target=\"paper226\">\n",
      "\"Multiscale simulation of cardiovascular flows on the IBM Bluegene/P: \n",
      "full heart-circulation system at near red-blood cell resolution\"</a>\n",
      "<br/> A. Peters, S. Melchionna, E. Kaxiras, J. Latt, J. Sircar, S. Succi, \n",
      "<i>2010 ACM/IEEE International Conference for High Performance </i>,\n",
      " doi: 10.1109/SC.2010.33 (2010).\n",
      "<br/>\n",
      "</li>\n",
      "</ol>\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "def parse_journal(index, publication): \n",
    "    if publication.find('i'): \n",
    "        journal = publication.i.text.lstrip().rstrip()\n",
    "        if journal == '':\n",
    "            print('Missing journal: ' + str(index))\n",
    "            print(publication)\n",
    "            \n",
    "        return journal\n",
    "    else:\n",
    "        print(\"Missing journal: \" + str(index))\n",
    "        print(publication)\n",
    "        return ''\n",
    "\n",
    "def parse_volume(index, publication): \n",
    "    if publication.find('b'):\n",
    "        volume = publication.b.text.lstrip().rstrip()\n",
    "        if volume == '':\n",
    "            print(\"Missing volume: \" + str(index))\n",
    "            print(publication)\n",
    "        return volume\n",
    "    else:\n",
    "        print(\"Missing volume: \" + str(index))\n",
    "        print(publication)\n",
    "        return ''\n",
    "\n",
    "def parse_url(index, publication):     \n",
    "    if publication.find('a'):\n",
    "        url = publication.a['href'].lstrip().rstrip()\n",
    "        if url == '': \n",
    "            print(\"Missing URL: \" + str(index))\n",
    "            print(publication)\n",
    "        return url\n",
    "    else:\n",
    "        print(\"Missing URL: \" + str(index))\n",
    "        print(publication)\n",
    "        return ''\n",
    "    \n",
    "def parse_title(index, publication):     \n",
    "    if publication.find('a'):\n",
    "        title = publication.a.text.lstrip().rstrip()\n",
    "        if title == '': \n",
    "            print(\"Missing Title: \" + str(index))\n",
    "            print(publication)       \n",
    "        return title\n",
    "    else:\n",
    "        print(\"Missing Title: \" + str(index))\n",
    "        print(publication)       \n",
    "        return ''\n",
    "\n",
    "def parse_author(index, publication):     \n",
    "    if publication.find('br'):\n",
    "        author = publication.br.next_sibling.lstrip().rstrip()\n",
    "        if author != '':\n",
    "            if author[-1] == ',':\n",
    "                author = author[:-1]\n",
    "        else:\n",
    "            print(\"Missing author: \" + str(index))\n",
    "            print(publication)                   \n",
    "        return author\n",
    "    else:\n",
    "        print(\"Missing author: \" + str(index))\n",
    "        print(publication)                   \n",
    "        return ''\n",
    "\n",
    "def parse_soup(soup):\n",
    "    results = []\n",
    "    for index, publication in enumerate(soup.find_all('ol')):\n",
    "        info = {}\n",
    "        info['journal'] = parse_journal(index, publication)\n",
    "        info['volume'] = parse_volume(index, publication)\n",
    "        info['URL'] = parse_url(index, publication)\n",
    "        info['title'] = parse_title(index, publication)\n",
    "        info['author'] = parse_author(index, publication)\n",
    "        \n",
    "        results.append(info)        \n",
    "    \n",
    "    return results\n",
    "\n",
    "articles = parse_soup(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3 Convert the list of dictionaries into the .bibTex format using python string manipulation (python string formatting on a template string is particularly useful)..**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "bib = ''\n",
    "for article in articles: \n",
    "    artical_str = '@article{\\n\\tauthor = \"%s\",\\n\\ttitle = %s,\\n\\tURL = \"%s\",\\n\\tjournal = \"%s\",\\n\\tvolume = %s\\n}' % (article['author'], article['title'], article['URL'], article['journal'], article['volume'])\n",
    "    bib = bib + artical_str + '\\n'\n",
    "\n",
    "with open('publist.bib', 'w') as f:\n",
    "    f.write(bib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check your answer - print the bibTex file\n",
    "# clear/remove output before making pdf\n",
    "f = open('publist.bib','r')\n",
    "# print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look like this\n",
    "```\n",
    "@article{    \n",
    "     author = \"Ming-Wei Lin, Cheng Ling, Luis A. Agapito, Nicholas Kioussis, Yiyang Zhang, Mark Ming-Cheng Cheng\",\n",
    "     title = \"Approaching the intrinsic band gap in suspended high-mobility graphene nanoribbons\",\n",
    "     URL = \"Papers/2011/PhysRevB_84_125411_2011.pdf\",\n",
    "     journal = \"PHYSICAL REVIEW B\",\n",
    "     volume = 84\n",
    "}\n",
    "\n",
    "...\n",
    "\n",
    "@article{    \n",
    "     author = \"E. Kaxiras and S. Succi\",\n",
    "     title = \"Multiscale simulations of complex systems: computation meets reality\",\n",
    "     URL = \"Papers/SciModSim_15_59_2008.pdf\",\n",
    "     journal = \"Sci. Model. Simul.\",\n",
    "     volume = 15\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1.4 Convert the list of dictionaries into the .csv format using pandas, and write the data into `publist.csv`. The csv file should have a header and no integer index...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you use head() when printing the dataframe\n",
    "# your code here\n",
    "df = pd.DataFrame(articles)\n",
    "df.to_csv('publist.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>author</th>\n",
       "      <th>journal</th>\n",
       "      <th>title</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Papers/2011/PhysRevB_84_125411_2011.pdf</td>\n",
       "      <td>Ming-Wei Lin, Cheng Ling, Luis A. Agapito, Nic...</td>\n",
       "      <td>PHYSICAL REVIEW B</td>\n",
       "      <td>\"Approaching the intrinsic band gap in suspend...</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Papers/2011/PhysRevB_84_035325_2011.pdf</td>\n",
       "      <td>JAdam Gali, Efthimios Kaxiras, Gergely T. Zima...</td>\n",
       "      <td>PHYSICAL REVIEW B</td>\n",
       "      <td>\"Effect of symmetry breaking on the optical ab...</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Papers/2011/PhysRevB_83_054204_2011.pdf</td>\n",
       "      <td>Jan M. Knaup, Han Li, Joost J. Vlassak, and Ef...</td>\n",
       "      <td>PHYSICAL REVIEW B</td>\n",
       "      <td>\"Influence of CH2 content and network defects ...</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Papers/2011/PhysRevB_83_045303_2011.pdf</td>\n",
       "      <td>Martin Heiss, Sonia Conesa-Boj, Jun Ren, Hsian...</td>\n",
       "      <td>PHYSICAL REVIEW B</td>\n",
       "      <td>\"Direct correlation of crystal structure and o...</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Papers/2011/PhilTransRSocA_369_2354_2011.pdf</td>\n",
       "      <td>Simone Melchionna, Efthimios Kaxiras, Massimo ...</td>\n",
       "      <td>Phil. Trans. R. Soc. A</td>\n",
       "      <td>\"Endothelial shear stress from large-scale blo...</td>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            URL                                             author                 journal                                              title volume\n",
       "0       Papers/2011/PhysRevB_84_125411_2011.pdf  Ming-Wei Lin, Cheng Ling, Luis A. Agapito, Nic...       PHYSICAL REVIEW B  \"Approaching the intrinsic band gap in suspend...     84\n",
       "1       Papers/2011/PhysRevB_84_035325_2011.pdf  JAdam Gali, Efthimios Kaxiras, Gergely T. Zima...       PHYSICAL REVIEW B  \"Effect of symmetry breaking on the optical ab...     84\n",
       "2       Papers/2011/PhysRevB_83_054204_2011.pdf  Jan M. Knaup, Han Li, Joost J. Vlassak, and Ef...       PHYSICAL REVIEW B  \"Influence of CH2 content and network defects ...     83\n",
       "3       Papers/2011/PhysRevB_83_045303_2011.pdf  Martin Heiss, Sonia Conesa-Boj, Jun Ren, Hsian...       PHYSICAL REVIEW B  \"Direct correlation of crystal structure and o...     83\n",
       "4  Papers/2011/PhilTransRSocA_369_2354_2011.pdf  Simone Melchionna, Efthimios Kaxiras, Massimo ...  Phil. Trans. R. Soc. A  \"Endothelial shear stress from large-scale blo...    369"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "PHYSICAL REVIEW B\n",
      "Phil. Trans. R. Soc. A\n",
      "New Journal of Physics\n",
      "Nano Lett.\n",
      "Langmuir\n",
      "J. Phys. Chem. Lett.\n",
      "J. Phys. Chem. C\n",
      "J. Chem. Phys.\n",
      "Chem. Eur. J.\n",
      "Catal. Sci. Technol.\n",
      "ACSNano.\n",
      "Acta Mater.\n",
      "New J. Phys.\n",
      "Phys. Rev. B\n",
      "2010 ACM/IEEE International Conference for High Performance\n",
      "Molec. Phys.\n",
      "Top. Catal.\n",
      "Phys. Rev. Lett.\n",
      "NanoLett.\n",
      "J. Chem. Theory Comput.\n",
      "Comp. Phys. Comm.\n",
      "Concurrency Computat.: Pract. Exper.\n",
      "Sol. St. Comm.\n",
      "Energy & Environmental Sci.\n",
      "Int. J. Cardiovasc. Imaging\n",
      "J. Stat. Mech: Th. and Exper.\n",
      "Phys. Rev. E - Rap. Comm.\n",
      "J. Phys. Chem. B\n",
      "Ab initio\n",
      "Sci. Model. Simul.\n"
     ]
    }
   ],
   "source": [
    "print(len(df.journal))\n",
    "\n",
    "for i in df.journal.unique():\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style='height:2px'>\n",
    "## Follow the stars in IMDb's list of \"The Top 100 Stars for 2017\" \n",
    "\n",
    "### Overview\n",
    "In this part, your goal is to extract information from IMDb's Top 100 Stars for 2017 (https://www.imdb.com/list/ls025814950/) and perform some analysis on each star in the list. In particular we are interested to know: a) how many performers made their first movie at 17? b) how many performers started as child actors? c) who is the most proliferate actress or actor in IMDb's list of the Top 100 Stars for 2017? . These questions are addressed in more details in the Questions below. \n",
    "\n",
    "When data is not given to us in a file, we need to fetch them using one of the following ways:\n",
    "- download a file from a source URL\n",
    "- query a database \n",
    "- query a web API \n",
    "- scrape data from the web page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 2 [52 pts]: Web Scraping using Beautiful Soup and exploring using Pandas </b></div>\n",
    "\n",
    "**2.1** Download the webpage of the \"Top 100 Stars for 2017\" (https://www.imdb.com/list/ls025814950/) into a `requests` object and name it `my_page`. Explain what the following attributes are:\n",
    "\n",
    "- `my_page.text`, \n",
    "- `my_page.status_code`,\n",
    "- `my_page.content`.\n",
    "\n",
    "**2.2** Create a Beautiful Soup object named `star_soup` using `my_page` as input.\n",
    "\n",
    "**2.3** Write a function called `parse_stars` that accepts `star_soup` as its input and generates a list of dictionaries named `starlist` (see definition below; order of dictionaries does not matter). One of the fields of this dictionary is the `url` of each star's individual page, which you need to scrape and save the contents in the `page` field. Note that there is a ton of information about each star on these webpages.\n",
    "\n",
    "```\n",
    "name: the name of the actor/actress as it appears at the top\n",
    "gender: 0 or 1: translate the word 'actress' into 1 and 'actor' into '0'\n",
    "url: the url of the link under their name that leads to a page with details\n",
    "page: BS object with html text acquired by scraping the above 'url' page' \n",
    "```\n",
    "\n",
    "**2.4** Write a function called `create_star_table` which takes `starlist` as an input and extracts information about each star (see function definition for the exact information to be extracted and the exact output definition).  Only extract information from the first box on each star's page. If the first box is acting, consider only acting credits and the star's acting debut, if the first box is Directing, consider only directing credits and directorial debut.\n",
    "\n",
    "**2.5** Now that you have scraped all the info you need, it's good practice to save the last data structure you created to disk. Save the data structure to a JSON file named `starinfo.json` and submit this JSON file in Canvas. If you do this, if you have to restart, you won't need to redo all the requests and parsings from before.  \n",
    "\n",
    "**2.6** We provide a JSON file called `data/staff_starinfo.json` created by CS109 teaching staff for consistency, which you should use for the rest of the homework. Import the contents of this JSON file  into a pandas dataframe called `frame`. Check the types of variables in each column and clean these variables if needed. Add a new column to your dataframe with the age of each actor when they made their first appearance, movie or TV, (name this column `age_at_first_movie`). Check some of the values of this new column. Do you find any problems? You don't need to fix them.\n",
    "\n",
    "**2.7** You are now ready to answer the following intriguing questions: \n",
    "- **2.7.1** How many performers made their first appearance (movie or TV) when he/she was 17 years old?\n",
    "\n",
    "- **2.7.2** How many performers started as child actors? Define child actor as a person younger than 12 years old. \n",
    "\n",
    "**2.8** Make a plot of the number of credits against the name of actor/actress. Who is the most prolific actress or actor in IMDb's list of the Top 100 Stars for 2017? Define **most prolific** as the performer with the most credits.\n",
    "    \n",
    "### Hints\n",
    "- Create a variable that groups actors/actresses by the age of their first movie. Use pandas' `.groupby` to divide the dataframe into groups of performers that for example started performing as children (age $<$ 12). The grouped variable is a `GroupBy` pandas object and this object has all of the information needed to then apply operations to each of the groups.\n",
    "- When cleaning the data make sure the variables with which you are performing calculations are in numerical format.\n",
    "- The column with the year has some values that are double, e.g. **'2000-2001'** and the column with age has some empty cells. You need to deal with these in a reasonable fashion before performing calculations on the data. \n",
    "- You should include both movies and TV shows.\n",
    "    \n",
    "### Resources\n",
    "- The `requests` library makes working with HTTP requests powerful and easy. For more on the `requests` library see http://docs.python-requests.org/\n",
    "\n",
    "### Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 Download the webpage of the \"Top 100 Stars for 2017 ...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Create a Beautiful Soup object named star_soup giving my_page as input.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check your code - you should see a familiar HTML page\n",
    "# clear/remove output before making pdf\n",
    "print (star_soup.prettify()[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 Write a function called `parse_stars` that accepts `star_soup` as its input ...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Function\n",
    "--------\n",
    "parse_stars\n",
    "\n",
    "Input\n",
    "------\n",
    "star_soup: the soup object with the scraped page\n",
    "   \n",
    "Returns\n",
    "-------\n",
    "a list of dictionaries; each dictionary corresponds to a star profile and has the following data:\n",
    "\n",
    "    name: the name of the actor/actress as it appears at the top\n",
    "    gender: 0 or 1: translate the word 'actress' into 1 and 'actor' into '0'\n",
    "    url: the url of the link under their name that leads to a page with details\n",
    "    page: BS object with 'html text acquired by scraping the above 'url' page' \n",
    "\n",
    "Example:\n",
    "--------\n",
    "{'name': Tom Hardy,\n",
    "  'gender': 0,\n",
    "  'url': https://www.imdb.com/name/nm0362766/?ref_=nmls_hd,\n",
    "  'page': BS object with 'html text acquired by scraping the 'url' page'\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should give you 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(starlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check your code\n",
    "# this list is large because of the html code into the `page` field\n",
    "# to get a better picture, print only the first element\n",
    "# clear/remove output before making pdf\n",
    "starlist[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look like this:\n",
    "```\n",
    "{'name': 'Gal Gadot',\n",
    " 'gender': 1,\n",
    " 'url': 'https://www.imdb.com/name/nm2933757?ref_=nmls_hd',\n",
    " 'page': \n",
    " <!DOCTYPE html>\n",
    " \n",
    " <html xmlns:fb=\"http://www.facebook.com/2008/fbml\" xmlns:og=\"http://ogp.me/ns#\">\n",
    " <head>\n",
    " <meta charset=\"utf-8\"/>\n",
    " <meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\"/>\n",
    " <meta content=\"app-id=342792525, app-argument=imdb:///name/nm2933757?src=mdot\" name=\"apple-itunes-app\"/>\n",
    " <script type=\"text/javascript\">var IMDbTimer={starttime: new Date().getTime(),pt:'java'};</script>\n",
    " <script>\n",
    "     if (typeof uet == 'function') {\n",
    "       uet(\"bb\", \"LoadTitle\", {wb: 1});\n",
    "     }\n",
    " </script>\n",
    " <script>(function(t){ (t.events = t.events || {})[\"csm_head_pre_title\"] = new Date().getTime(); })(IMDbTimer);</script>\n",
    " \n",
    "... \n",
    "\n",
    "\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4 Write a function called `create_star_table` to extract information about each star ...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Function\n",
    "--------\n",
    "create_star_table\n",
    "\n",
    "Input\n",
    "------\n",
    "the starlist\n",
    "   \n",
    "Returns\n",
    "-------\n",
    "\n",
    "a list of dictionaries; each dictionary corresponds to a star profile and has the following data:\n",
    "\n",
    "    star_name: the name of the actor/actress as it appears at the top\n",
    "    gender: 0 or 1 (1 for 'actress' and 0 for 'actor')  \n",
    "    year_born : year they were born\n",
    "    first_movie: title of their first movie or TV show\n",
    "    year_first_movie: the year they made their first movie or TV show\n",
    "    credits: number of movies or TV shows they have made in their career.\n",
    "    \n",
    "--------\n",
    "Example:\n",
    "\n",
    "{'star_name': Tom Hardy,\n",
    "  'gender': 0,\n",
    "  'year_born': 1997,\n",
    "  'first_movie' : 'Batman',\n",
    "  'year_first_movie' : 2017,\n",
    "  'credits' : 24}\n",
    "  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_star_table(starlist: list) -> list:\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check your code\n",
    "# clear/remove output before making the pdf file\n",
    "star_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look like this (the order of elements is not important):\n",
    "```\n",
    "[{'name': 'Gal Gadot',\n",
    "  'gender': 1,\n",
    "  'year_born': '1985',\n",
    "  'first_movie': 'Bubot',\n",
    "  'year_first_movie': '2007',\n",
    "  'credits': '25'},\n",
    " {'name': 'Tom Hardy',\n",
    "  'gender': 0,\n",
    "  'year_born': '1977',\n",
    "  'first_movie': 'Tommaso',\n",
    "  'year_first_movie': '2001',\n",
    "  'credits': '55'},\n",
    "  \n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5 Now that you have scraped all the info you need, it's a good practice to save the last data structure you ...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check your JSON saving, re-open the JSON file and reload the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"starinfo.json\", \"r\") as fd:\n",
    "    star_table = json.load(fd)\n",
    "    \n",
    "# output should be the same\n",
    "# clear/remove output before making the pdf file\n",
    "star_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6 Import the contents of the staff's JSON file (`data/staff_starinfo.json`) into a pandas dataframe. ...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.7 You are now ready to answer the following intriguing questions**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.7.1 How many performers made their first movie at 17?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look like this:<BR>\n",
    "8 performers made their first movie at 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.7.2 How many performers started as child actors? Define child actor as a person less than 12 years old.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.8 Make a plot of the number of credits versus the name of actor/actress.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px\">\n",
    "##  Going the Extra Mile \n",
    "Be sure to complete problems 1 and 2 before tackling this problem...it is worth only 8 points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b>Question 3 [8 pts]: Parsing using Regular Expressions (regex) </b></div>\n",
    "\n",
    "Even though scraping HTML with regex is sometimes considered bad practice, you are to use python's **regular expressions** to answer this problem.  Regular expressions are useful to parse strings, text, tweets, etc. in general (for example, you may encounter a non-standard format for dates at some point). Do not use BeautifulSoup to answer this problem.\n",
    "\n",
    " **3.1** Write a function called `get_pubs` that takes an .html filename as an input and returns a string containing the HTML page in this file (see definition below). Call this function using `data/publist_super_clean.html` as input and name the returned string `prof_pubs`. \n",
    " \n",
    " **3.2** Calculate how many times the author named '`C.M. Friend`' appears in the list of publications. \n",
    " \n",
    " **3.3** Find all unique journals and copy them in a variable named `journals`.  \n",
    " \n",
    " **3.4** Create a list named `pub_authors` whose elements are strings containing the authors' names for each paper. \n",
    "    \n",
    "### Hints\n",
    "- Look for patterns in the HTML tags that reveal where each piece of information such as the title of the paper, the names of the authors, the journal name, is stored. For example, you might notice that the journal name(s) is contained between the &lt;I&gt; HTML tag. \n",
    "- Learning about your domain is always a good idea: you want to check the names to make sure that they belong to actual journals. Thus, while journal name(s) is contained between the &lt;I&gt; HTML tag, please note that <i>all</i> strings found between &lt;I&gt; tags may not be journal names.\n",
    "- Each publication has multiple authors. \n",
    "- `C.M. Friend` also shows up as `Cynthia M. Friend` in the file.  Count just `C. M. Friend`. \n",
    "- There is a comma at the end of the string of authors. You can choose to keep it in the string or remove it and put it back when you write the string as a BibTex entry. \n",
    "- You want to remove duplicates from the list of journals. Duplicates may also occur due to misspellings or spaces, such as: `Nano Lett.`, and `NanoLett.` You can assume that any journals with the same initials (e.g., `NL` for `NanoLett.`) are the same journal.\n",
    "\n",
    "### Resources\n",
    "- **Regular expressions:** a) https://docs.python.org/3.3/library/re.html, b) https://regexone.com, and c) https://docs.python.org/3/howto/regex.html. \n",
    "- ** HTML:** if you are not familiar with HTML see https://www.w3schools.com/html/ or one of the many tutorials on the internet. \n",
    "- ** Document Object Model (DOM):** for more on this programming interface for HTML and XML documents see https://www.w3schools.com/js/js_htmldom.asp. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first import the necessary reg expr library\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this file provided\n",
    "PUB_FILENAME = 'data/publist_super_clean.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "def get_pubs(filehtml):\n",
    "    with open(filehtml) as fdr:\n",
    "        data = fdr.read()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "prof_pubs = get_pubs(PUB_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# checking your code \n",
    "# clear/remove output before creating the pdf file\n",
    "# print(prof_pubs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see an HTML page that looks like this (colors are not important)\n",
    "```html\n",
    "<LI>\n",
    "<A HREF=\"Papers/2011/PhysRevB_84_125411_2011.pdf\" target=\"paper244\">\n",
    "&quot;Approaching the intrinsic band gap in suspended high-mobility graphene nanoribbons&quot;</A>\n",
    "<BR>Ming-Wei Lin, Cheng Ling, Luis A. Agapito, Nicholas Kioussis, Yiyang Zhang, Mark Ming-Cheng Cheng,\n",
    "<I>PHYSICAL REVIEW B </I> <b>84</b>,  125411 (2011)\n",
    "<BR>\n",
    "</LI>\n",
    "</OL>\n",
    "\n",
    "<OL START=243>\n",
    "<LI>\n",
    "<A HREF=\"Papers/2011/PhysRevB_84_035325_2011.pdf\" target=\"paper243\">\n",
    "&quot;Effect of symmetry breaking on the optical absorption of semiconductor nanoparticles&quot;</A>\n",
    "<BR>JAdam Gali, Efthimios Kaxiras, Gergely T. Zimanyi, Sheng Meng,\n",
    "<I>PHYSICAL REVIEW B </I> <b>84</b>,  035325 (2011)\n",
    "<BR>\n",
    "</LI>\n",
    "</OL>\n",
    "\n",
    "<OL START=242>\n",
    "<LI>\n",
    "<A HREF=\"Papers/2011/PhysRevB_83_054204_2011.pdf\" target=\"paper242\">\n",
    "&quot;Influence of CH2 content and network defects on the elastic properties of organosilicate glasses&quot;</A>\n",
    "<BR>Jan M. Knaup, Han Li, Joost J. Vlassak, and Efthimios Kaxiras,\n",
    "<I>PHYSICAL REVIEW B </I> <b>83</b>,  054204 (2011)\n",
    "<BR>\n",
    "</LI>\n",
    "</OL>\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 Calculate how many times the author ... **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C.M. Friend appears 5 times.\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "CMF_num = len(re.findall(r\"C.M. Friend\", prof_pubs))\n",
    "print(\"C.M. Friend appears %d times.\" % CMF_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3 Find all unique journals and copy ...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here \n",
    "journals = re.findall(r\"(?<=<I>).*(?=</I>)\", prof_pubs)\n",
    "journals = set(journals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to domain knowledge, 'Ab initio' is not a journal, so drop it. \n",
    "journals.remove('Ab initio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "df = pd.DataFrame({'raw': list(journals)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='raw', inplace=True)\n",
    "df['init'] = df['raw'].apply(lambda x: ''.join([c for c in x if c.isupper()]))\n",
    "df['dup'] = df.duplicated(subset=['init'])\n",
    "df = df[df['dup']==False]\n",
    "\n",
    "df['split'] = df['raw'].apply(lambda x: ''.join([word[0] for word in x.split()]))\n",
    "df['dup2'] = df.duplicated(subset=['split'])\n",
    "df = df[df['dup2']==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2010 ACM/IEEE International Conference for High Performance ',\n",
       " 'ACSNano. ',\n",
       " 'Acta Mater. ',\n",
       " 'Catal. Sci. Technol. ',\n",
       " 'Chem. Eur. J. ',\n",
       " 'Comp. Phys. Comm. ',\n",
       " 'Concurrency Computat.: Pract. Exper. ',\n",
       " 'Energy & Environmental Sci. ',\n",
       " 'Int. J. Cardiovasc. Imaging ',\n",
       " 'J. Chem. Phys. ',\n",
       " 'J. Chem. Theory Comput. ',\n",
       " 'J. Phys. Chem. B ',\n",
       " 'J. Phys. Chem. C ',\n",
       " 'J. Phys. Chem. Lett. ',\n",
       " 'J. Stat. Mech: Th. and Exper. ',\n",
       " 'Langmuir ',\n",
       " 'Molec. Phys. ',\n",
       " 'Nano Lett. ',\n",
       " 'New J. Phys. ',\n",
       " 'PHYSICAL REVIEW B ',\n",
       " 'Phil. Trans. R. Soc. A ',\n",
       " 'Phys. Rev. E - Rap. Comm. ',\n",
       " 'Phys. Rev. Lett. ',\n",
       " 'Sci. Model. Simul. ',\n",
       " 'Sol. St. Comm. ',\n",
       " 'Top. Catal. '}"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check your code \n",
    "journals = set(df['raw'])\n",
    "journals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look like this (no duplicates):\n",
    "```\n",
    "{'2010 ACM/IEEE International Conference for High Performance',\n",
    " 'ACSNano.',\n",
    " 'Acta Mater.',\n",
    " 'Catal. Sci. Technol.',\n",
    " 'Chem. Eur. J.',\n",
    " 'Comp. Phys. Comm.',\n",
    " 'Concurrency Computat.: Pract. Exper.',\n",
    " 'Energy & Environmental Sci.',\n",
    " 'Int. J. Cardiovasc. Imaging',\n",
    " 'J. Chem. Phys.',\n",
    " 'J. Chem. Theory Comput.',\n",
    " 'J. Phys. Chem. B',\n",
    " 'J. Phys. Chem. C',\n",
    " 'J. Phys. Chem. Lett.',\n",
    " 'J. Stat. Mech: Th. and Exper.',\n",
    " 'Langmuir',\n",
    " 'Molec. Phys.',\n",
    " 'Nano Lett.',\n",
    " 'New Journal of Physics',\n",
    " 'PHYSICAL REVIEW B',\n",
    " 'Phil. Trans. R. Soc. A',\n",
    " 'Phys. Rev. E - Rap. Comm.',\n",
    " 'Phys. Rev. Lett.',\n",
    " 'Sci. Model. Simul.',\n",
    " 'Sol. St. Comm.',\n",
    " 'Top. Catal.'}\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4 Create a list named `pub_authors`...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "raw_authors = re.findall(r\"(?<=<BR>).{5,}(?=\\n)\", prof_pubs)\n",
    "pub_authors = [x.lstrip().rstrip() for x in raw_authors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ming-Wei Lin, Cheng Ling, Luis A. Agapito, Nicholas Kioussis, Yiyang Zhang, Mark Ming-Cheng Cheng,\n",
      "JAdam Gali, Efthimios Kaxiras, Gergely T. Zimanyi, Sheng Meng,\n",
      "Jan M. Knaup, Han Li, Joost J. Vlassak, and Efthimios Kaxiras,\n",
      "Martin Heiss, Sonia Conesa-Boj, Jun Ren, Hsiang-Han Tseng, Adam Gali,\n",
      "Simone Melchionna, Efthimios Kaxiras, Massimo Bernaschi and Sauro Succi,\n",
      "J R Maze, A Gali, E Togan, Y Chu, A Trifonov,\n",
      "Kejie Zhao, Wei L. Wang, John Gregoire, Matt Pharr, Zhigang Suo,\n",
      "Masataka Katono, Takeru Bessho, Sheng Meng, Robin Humphry-Baker, Guido Rothenberger,\n",
      "Thomas D. Kuhne, Tod A. Pascal, Efthimios Kaxiras, and Yousung Jung,\n",
      "Sheng Meng, Efthimios Kaxiras, Md. K. Nazeeruddin, and Michael Gratzel,\n",
      "Bingjun Xu, Jan Haubrich, Thomas A. Baker, Efthimios Kaxiras, and Cynthia M. Friend,\n",
      "Jun Ren, Sheng Meng, Yi-Lin Wang, Xu-Cun Ma, Qi-Kun Xue, Efthimios Kaxiras,\n",
      "Jan Haubrich, Efthimios Kaxiras, and Cynthia M. Friend,\n",
      "Thomas A. Baker, Bingjun Xu, Stephen C. Jensen, Cynthia M. Friend and Efthimios Kaxiras,\n",
      "Youdong Mao, Wei L. Wang, Dongguang Wei, Efthimios Kaxiras, and Joseph G. Sodroski,\n",
      "H. Li, J.M. Knaup, E. Kaxiras and J.J. Vlassak,\n",
      "W.L. Wang and E. Kaxiras,\n",
      "L.A. Agapito, N. Kioussis and E. Kaxiras,\n",
      "A. Peters, S. Melchionna, E. Kaxiras, J. Latt, J. Sircar, S. Succi,\n",
      "J. Ren, E. Kaxiras and S. Meng,\n",
      "T.A. Baker, E. Kaxiras and C.M. Friend,\n",
      "H.P. Chen, R.K. Kalia, E. Kaxiras, G. Lu, A. Nakano, K. Nomura,\n",
      "S. Meng and E. Kaxiras,\n",
      "C.L. Chang, S.K.R.S. Sankaranarayanan, D. Ruzmetov, M.H. Engelhard, E. Kaxiras and S. Ramanathan,\n",
      "T.A. Baker, C.M. Friend and E. Kaxiras,\n",
      "S. Melchionna, M. Bernaschi, S. Succi, E. Kaxiras, F.J. Rybicki, D. Mitsouras, A.U. Coskun and C.L. Feldman,\n",
      "M. Bernaschi, M. Fatica, S. Melchionna, S. Succi and E. Kaxiras,\n",
      "E. Manousakis, J. Ren, S. Meng and E. Kaxiras,\n",
      "A. Gali, E. Janzen, P. Deak, G. Kresse and E. Kaxiras,\n",
      "S.K.R.S. Sankaranarayanan, E. Kaxiras and S. Ramanathan,\n",
      "M. Bernaschi, S. Melchionna, S. Succi, M. Fyta, E. Kaxiras\n",
      "T.A. Baker, B.J. Xu, X.Y. Liu, E. Kaxiras and C.M. Friend,\n",
      "F.J. Rybicki, S. Melchionna, D. Mitsouras, A.U. Coskun, A.G. Whitmore, E. Kaxiras, S. Succi, P.H. Stone and C.L. Feldman,\n",
      "H. Chen, W.G. Zhu, E. Kaxiras, and Z.Y. Zhang,\n",
      "M. Fyta, S. Melchionna, M. Bernaschi, E. Kaxiras and S. Succi,\n",
      "E.M. Kotsalis, J.H. Walther, E. Kaxiras and P. Koumoutsakos,\n",
      "C.E. Lekka, J. Ren, S. Meng and E. Kaxiras,\n",
      "W.L. Wang, O.V. Yazyev, S. Meng and E. Kaxiras,\n",
      "A. Gali and E. Kaxiras,\n",
      "S. Melchionna, M. Bernaschi, M. Fyta, E. Kaxiras and S. Succi,\n",
      "S.K.R.S. Sankaranarayanan, E. Kaxiras, S. Ramanathan,\n",
      "T.A. Baker, C.M. Friend and E. Kaxiras,\n",
      "T.A. Baker, C.M. Friend and E. Kaxiras,\n",
      "E. Kaxiras and S. Succi,\n",
      "E. Manousakis, J. Ren, S. Meng and E. Kaxiras,\n"
     ]
    }
   ],
   "source": [
    "# check your code: print the list of strings containing the author(s)' names\n",
    "for item in pub_authors:\n",
    "    print (item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look like this (a line for each paper's authors string of names)\n",
    "```\n",
    "Ming-Wei Lin, Cheng Ling, Luis A. Agapito, Nicholas Kioussis, Yiyang Zhang, Mark Ming-Cheng Cheng,\n",
    "JAdam Gali, Efthimios Kaxiras, Gergely T. Zimanyi, Sheng Meng,\n",
    "Jan M. Knaup, Han Li, Joost J. Vlassak, and Efthimios Kaxiras,\n",
    "Martin Heiss, Sonia Conesa-Boj, Jun Ren, Hsiang-Han Tseng, Adam Gali,\n",
    "\n",
    "...\n",
    "\n",
    "T.A. Baker, C.M. Friend and E. Kaxiras,\n",
    "T.A. Baker, C.M. Friend and E. Kaxiras,\n",
    "E. Kaxiras and S. Succi,\n",
    "E. Manousakis, J. Ren, S. Meng and E. Kaxiras,\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
